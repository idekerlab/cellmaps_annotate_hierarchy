{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26f6ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ···················································\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5851615a-1815-4898-ab51-6da23bdc72ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c17ff7-d19b-455e-a55d-30367f6bc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the API key from an environment variable or secret management service\n",
    "openai.api_key = key; #os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def chatgpt_query_to_text(prompt,\n",
    "                          model=\"gpt-4\",\n",
    "                         #  engine=\"davinci\",\n",
    "                         # max_tokens=2000,\n",
    "                          n=1,\n",
    "                          stop=None,\n",
    "                          temperature=0):\n",
    "    # Call the OpenAI API to generate answers\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        # engine=engine,\n",
    "        prompt=prompt,\n",
    "        #max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        stop=stop,\n",
    "        temperature=temperature)\n",
    "    print(response)\n",
    "    response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a50bc-e6ee-4839-9c31-8374f5357baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8028f78-4c23-49c4-8821-80eeeb9f31db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ca43b0-2945-4551-bcd2-ff3649f0ed12",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text2 \u001b[38;5;241m=\u001b[39m chatgpt_query_to_text(prompt)\n",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mchatgpt_query_to_text\u001b[0;34m(prompt, model, n, stop, temperature)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatgpt_query_to_text\u001b[39m(prompt,\n\u001b[1;32m      6\u001b[0m                           model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                          \u001b[38;5;66;03m#  engine=\"davinci\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                           temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Call the OpenAI API to generate answers\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     14\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# engine=engine,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#max_tokens=max_tokens,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m     19\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m     20\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     22\u001b[0m     response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/annotate_hierarchy/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/annotate_hierarchy/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/annotate_hierarchy/lib/python3.11/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/annotate_hierarchy/lib/python3.11/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    626\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/annotate_hierarchy/lib/python3.11/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "text2 = chatgpt_query_to_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86991b1-342b-49ac-9ca1-ba4ef9b3a8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d57a08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" and the protein it encodes.\\n\\nThe RBL2 gene is located on chromosome 1. It is a member of the RBL gene family. The RBL2 gene encodes a protein called RBL2 protein. The RBL2 protein is a member of the RBL family of proteins. The RBL2 protein is a protein kinase. It is a serine/threonine protein kinase. It is a member of the RBL family of serine/threonine protein kinases. The RBL2 protein is a protein kinase that is involved in the regulation of the cell cycle. It is involved in the regulation of the G1/S transition. It is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the retinoblastoma protein. The RBL2 protein is involved in the regulation of the G1/S transition by phosphorylating the\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1684187700,\n",
      "  \"id\": \"cmpl-7GaRgA3hAt0x8iWZicQrE2ondbfbu\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1000,\n",
      "    \"prompt_tokens\": 10,\n",
      "    \"total_tokens\": 1010\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = chatgpt_query_to_text(\"Write a brief summary about the RBL2 gene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cef3d4-9db5-45ae-9c76-107fc33ad825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18595caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f630c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    #model=\"gpt-3.5-turbo\",\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344501a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "LOG_FILE = \"log.json\"\n",
    "DOLLAR_LIMIT = 1.0  # Set your dollar limit here\n",
    "\n",
    "def load_log():\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return {\"tokens_used\": 0, \"dollars_spent\": 0.0}\n",
    "\n",
    "def save_log(log_data):\n",
    "    with open(LOG_FILE, \"w\") as f:\n",
    "        json.dump(log_data, f, indent=4)\n",
    "\n",
    "def estimate_cost(tokens, rate_per_token):\n",
    "    return tokens * rate_per_token\n",
    "\n",
    "def openai_chat(prompt, model, max_tokens, rate_per_token):\n",
    "    log_data = load_log()\n",
    "    tokens_estimate = len(prompt) + max_tokens\n",
    "\n",
    "    if estimate_cost(log_data[\"tokens_used\"] + tokens_estimate, rate_per_token) > DOLLAR_LIMIT:\n",
    "        print(\"The API call is estimated to exceed the dollar limit. Aborting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "        tokens_used = response[\"choices\"][0][\"usage\"][\"total_tokens\"]\n",
    "\n",
    "        # Update and save the log\n",
    "        log_data[\"tokens_used\"] += tokens_used\n",
    "        log_data[\"dollars_spent\"] = estimate_cost(log_data[\"tokens_used\"], rate_per_token)\n",
    "        save_log(log_data)\n",
    "\n",
    "        return response[\"choices\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"What are some tips for productivity?\"\n",
    "    model = \"text-davinci-002\"\n",
    "    max_tokens = 50\n",
    "    rate_per_token = 0.0005  # Set your rate per token (in dollars)\n",
    "\n",
    "    response_text = openai_chat(prompt, model, max_tokens, rate_per_token)\n",
    "    if response_text:\n",
    "        print(response_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotate_hierarchy",
   "language": "python",
   "name": "annotate_hierarchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
